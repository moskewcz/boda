CUCL_GLOBAL_KERNEL void %(rtc_func_name)( GASQ float const * const a, // CUCL IN K:M
					  GASQ float const * const b, // CUCL IN K:N
					  GASQ float * const c )  // CUCL OUT M:N
/* work */  // CUCL REF Mg:Ng:Mb:Nb:Kb:Mt:Nt
{
  // CUCL IX GRP_ID_1D work use_dims=Mg:Ng
  // CUCL IX LOC_ID_1D work use_dims=Mb:Nb
  // note: <each thread handles> work use_dims=Mt:Nt output points;loops over K with unrollingfactor of Kb
  // FIXME: for now, we assume Kb == 1
  LOCSHAR_MEM float%(vw) a_sm[%(a_sm_sz)];
  LOCSHAR_MEM float%(vw) b_sm[%(b_sm_sz)];

  float c_r[%(work_Mt_dim)*%(work_Nt_dim)] = {0}; // tile of output for this thread to compute, stored in registers
  float%(vw) a_r[%(work_Mt_dim)/%(vw)]; 
  float%(vw) b_r[%(work_Nt_dim)/%(vw)];

  int32_t a_off = %(GRP_ID_1D_Mg)*%(work_Mb_dim)*%(work_Mt_dim)*%(a_M_stride)/%(vw) + LOC_ID_1D;
  int32_t b_off = %(GRP_ID_1D_Ng)*%(work_Nb_dim)*%(work_Nt_dim)*%(b_N_stride)/%(vw) + LOC_ID_1D;

  LSMASQ float%(vw) * const a_sm_off = a_sm + %(LOC_ID_1D_Mb)*%(work_Mt_dim)/%(vw);
  LSMASQ float%(vw) * const b_sm_off = b_sm + %(LOC_ID_1D_Nb)*%(work_Nt_dim)/%(vw);

  for( int32_t k = 0; k < %(a_K_dim); k += %(work_Kb_dim) ) {
    %(sm_loads);
    BARRIER_SYNC;
    %(inner_loop_body);
    a_off += %(work_Kb_dim)*%(a_K_stride)/%(vw);
    b_off += %(work_Kb_dim)*%(b_K_stride)/%(vw);
    BARRIER_SYNC;
  }

  int32_t c_off = // thread-level offset into c
    (%(GRP_ID_1D_Mg)*%(work_Mb_dim)+%(LOC_ID_1D_Mb))*%(work_Mt_dim)*%(c_M_stride)/%(vw) + 
    (%(GRP_ID_1D_Ng)*%(work_Nb_dim)+%(LOC_ID_1D_Nb))*%(work_Nt_dim)*%(c_N_stride)/%(vw);

  for( int32_t Mt = 0; Mt < %(work_Mt_dim); ++Mt ) {
    %(outs_to_b_r);
    %(stores);
    c_off += %(c_M_stride)/%(vw);
  }

}
