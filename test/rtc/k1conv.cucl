CUCL_GLOBAL_KERNEL void %(rtc_func_name)( GASQ float const * const filts, // CUCL IN out_chan_blk:in_chan:y:x:out_chan_reg:out_chan_tile
					  GASQ float const * const biases, // CUCL IN out_chan
					  GASQ float const * const in, // CUCL IN blk:blk_iter:blk_iter_chan:blk_pel
  // note: there are two possible output formats, so there are two allowed/valid dims for out:--v          OR ---v
					  GASQ float * const out, // CUCL OUT                  img:chan:y:x     blk:blk_iter:blk_iter_chan:blk_pel
					  int32_t const flags )
// yeah, okay, we don't use stride/in_pad here. but in codegen, we
// check that the stride is really 1, so we must declare then here.
/* stride */  // CUCL REF y:x 
/* in_pad */  // CUCL REF y:x 
/* work */  // CUCL REF pels_blk:out_chan_blk:pels_tile:out_chan_tile:pels:out_chan
/* out_ref */  // CUCL REF img:chan:y:x
{
  // CUCL IX out_ref_pel out_ref use_dims=img:y:x 
  // CUCL IX GRP_ID_1D work use_dims=pels_blk:out_chan_blk
  // CUCL IX LOC_ID_1D work use_dims=pels_tile:out_chan_tile
  // note: <each thread handles> work use_dims=pels:out_chan; with pels_sz==out_chan_sz==t_tile_sz (currently); loops over in.chan==filts.in_chan
  // note: for k1conv we have filts_y_dim==filts_x_dim==1
  // refactoring note: blk_filt_ix_sz --> %(filts_x_sz) == (number of output chans handled by this block) (also == %(filts_y_sz) and %(filts_in_chan_sz))
  // refactoring note: in_chan_tile --> %(in_blk_iter_chan_dim) 
  // note: all_smem_sz is max(filts+in,out) == max((%(filts_in_chan_sz)*%(in_blk_iter_chan_dim))+%(in_blk_iter_sz),%(out_smem_sz))
  LOCSHAR_MEM float all_smem[%(all_smem_sz)]; 
  LSMASQ float * const filts_smem = all_smem;
  LSMASQ float * const in_smem = filts_smem + %(filts_in_chan_sz)*%(in_blk_iter_chan_dim); // aka "+ filts_smem_sz"

  float out_tile[%(work_pels_dim)*%(work_out_chan_dim)] = {0}; // tile of output for this thread to compute, stored in registers
  // reg. buffers for one strip each from in and filts, for the same filts_ix_out_chan_elem
  float filts_strip[%(work_out_chan_dim)]; // across output chans (stride is blk_filt_ix_sz )
  float in_strip[%(work_pels_dim)]; // across pels (approx square block in x/y space, favoring x if sqrt() not integer)

  int32_t const blk_filt_ix_base = %(GRP_ID_1D_out_chan_blk)*%(filts_out_chan_blk_sz); // index of first out chan
  int32_t blk_in_ix_base = %(GRP_ID_1D_pels_blk)*%(in_blk_sz) + LOC_ID_1D;// index of first input pel to load for this thread

  LSMASQ float * const filts_smem_off = filts_smem + %(LOC_ID_1D_out_chan_tile);
  LSMASQ float * const in_smem_off = in_smem + %(LOC_ID_1D_pels_tile)*%(work_pels_dim);
  LSMASQ float * const out_smem_off = all_smem + LOC_ID_1D;
  int32_t filts_off = blk_filt_ix_base + LOC_ID_1D;
  // iteratate over filter elements
  for( int32_t blk_iter = 0; blk_iter != %(in_blk_iter_dim); ++blk_iter ) {
    BARRIER_SYNC;
    %(filts_smem_loads);
    %(smem_loads);
    BARRIER_SYNC;
    filts_off += %(filts_in_chan_sz)*%(in_blk_iter_chan_dim);
    blk_in_ix_base += %(in_blk_iter_sz);
    %(inner_loop_body);
  }
  // load per-block biases into smem
  if( flags == 2 ) { return; }
  BARRIER_SYNC;
  %(biases_smem_loads);
  BARRIER_SYNC;
  // load biases into filts_strip
  %(bias_loads);
  if( flags == 1 ) { 
    GASQ float * const out_off = out + LOC_ID_1D;
    %(dummy_stores);
    return; 
  }
  // add bias to each elem of out_tile[] and store the results to out[]
  %(stores);
}

